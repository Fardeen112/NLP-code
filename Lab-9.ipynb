{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings:\n",
      "tensor([[[-0.0242, -0.0613, -0.3574,  ..., -0.2367,  0.1170,  0.6796],\n",
      "         [ 0.4344,  0.1215, -0.5670,  ..., -0.1856,  0.7115,  0.6390],\n",
      "         [-0.0379,  0.6720,  0.2160,  ..., -0.0414,  0.1997,  0.7380],\n",
      "         ...,\n",
      "         [ 0.2783,  0.7914, -0.2554,  ..., -0.2284,  0.0477, -0.0451],\n",
      "         [ 0.5973,  0.0904, -0.3479,  ...,  0.3831, -0.4819, -0.2906],\n",
      "         [ 0.8469,  0.1153, -0.1804,  ...,  0.3553, -0.6829, -0.1788]]])\n",
      "Sentence Embedding:\n",
      "tensor([ 2.2410e-01,  3.2297e-01, -2.5053e-01, -1.1223e-01,  1.6730e-01,\n",
      "        -4.0618e-01,  1.1602e-01,  7.7370e-01,  1.3931e-01, -1.5208e-01,\n",
      "        -2.2460e-02, -1.4040e-02, -1.7763e-02,  4.0001e-02, -1.7007e-01,\n",
      "         1.7948e-02,  2.4343e-01,  2.0567e-01, -3.2807e-01,  3.9778e-01,\n",
      "         4.2182e-01,  2.2755e-01, -3.4791e-01,  3.8017e-01,  5.2279e-01,\n",
      "        -1.8999e-01, -2.6396e-01,  8.9271e-02, -1.6254e-01, -3.9519e-01,\n",
      "        -3.9775e-02, -1.4239e-02, -1.9080e-01,  8.3151e-02, -1.4556e-01,\n",
      "        -7.4988e-02, -3.2242e-01, -2.3643e-01, -2.8300e-01,  6.3122e-02,\n",
      "        -2.2291e-01, -6.5727e-03, -2.0547e-01,  2.4662e-02, -3.8092e-01,\n",
      "        -9.5320e-02, -2.4312e-01,  1.2509e-02,  4.4374e-01,  1.0129e-01,\n",
      "        -2.0733e-01,  3.1222e-01,  6.5967e-02,  1.3061e-01,  4.6795e-02,\n",
      "         6.9577e-01, -2.8210e-01, -4.8478e-01, -4.9008e-02, -8.0576e-02,\n",
      "         1.4602e-01, -9.7753e-03, -4.0384e-02, -5.0641e-01,  2.6671e-01,\n",
      "         1.4548e-01, -1.1043e-01, -2.5453e-01, -5.0722e-01,  2.6591e-04,\n",
      "        -2.0140e-01, -3.1594e-01,  1.9946e-01,  3.4617e-02, -2.9180e-01,\n",
      "         2.9757e-01, -8.7879e-02,  8.4801e-02, -4.9861e-02, -7.5922e-02,\n",
      "        -1.6476e-01,  6.3880e-01,  1.8202e-01,  3.8914e-01,  4.9994e-01,\n",
      "         2.6584e-01, -6.5155e-01,  1.5979e-02, -2.1920e-01, -3.7477e-01,\n",
      "        -7.5613e-02, -8.5975e-02, -2.2733e-01,  3.5161e-01,  6.0498e-01,\n",
      "         1.2605e-01, -1.0113e-01, -1.2584e-01, -2.9647e-01,  3.7141e-01,\n",
      "         9.8574e-02, -5.5498e-01,  1.7368e-01,  5.1697e-01,  3.8866e-01,\n",
      "         2.8297e-01,  1.1188e-02,  5.1969e-02,  1.6156e-01, -1.0025e-01,\n",
      "        -1.6358e-01, -3.6205e-01, -3.5565e-02, -5.5652e-01, -5.2133e-01,\n",
      "        -2.0071e-01,  2.8368e-01,  3.9155e-01, -2.4283e-02, -5.1012e-02,\n",
      "        -2.7090e-01, -1.0909e-01,  2.5785e-01,  1.0724e+00, -1.8545e-01,\n",
      "        -1.2686e-01, -5.1376e-02, -2.6213e-01,  2.8871e-02, -5.6132e-01,\n",
      "         2.7746e-01,  7.0974e-01,  6.3489e-01, -1.4651e-01, -4.7512e-01,\n",
      "         1.3991e-01,  2.4399e-01, -3.6947e-01, -6.3883e-01,  1.8084e-01,\n",
      "        -1.1781e-01,  1.3993e-01,  3.0178e-02, -1.7405e-01,  2.6113e-01,\n",
      "         5.3562e-04, -3.5882e-02,  1.8024e-01,  3.8898e-01,  9.2242e-02,\n",
      "         3.1364e-01,  2.3850e-01, -3.8978e-01, -3.1355e-01, -3.1856e-01,\n",
      "         3.2241e-02, -4.2422e-01,  1.8499e-01,  7.2845e-01,  9.4603e-02,\n",
      "        -1.5245e-01, -2.1808e-01, -4.0929e-01,  7.9569e-02, -2.1276e-01,\n",
      "         2.1668e-01,  6.0040e-02,  7.0000e-01, -1.4396e-01,  5.5384e-02,\n",
      "        -2.0871e-01, -7.1365e-02,  7.9524e-01, -1.4064e-01, -5.0949e-01,\n",
      "         1.1372e-01, -8.7214e-02,  4.2927e-01,  4.6358e-01,  1.6697e-01,\n",
      "        -2.0144e+00,  9.4451e-02,  3.0901e-01, -9.1830e-02,  1.2468e-01,\n",
      "        -3.7752e-01,  2.3269e-01, -6.0280e-01, -4.7036e-01, -1.2103e-02,\n",
      "        -1.3659e-02,  1.2942e-01, -2.1458e-01, -1.7863e-01,  4.4759e-01,\n",
      "        -1.6478e-01, -2.0964e-01,  1.4784e-01,  1.3342e-01,  2.5906e-01,\n",
      "        -1.4849e-02, -1.4958e-01,  5.8624e-02,  2.6458e-01,  3.5511e-02,\n",
      "        -2.1324e-02,  1.9747e-01,  2.4398e-01, -2.1925e-01,  6.2994e-03,\n",
      "        -3.3215e-01,  6.2474e-01,  1.2634e-01, -3.0462e-02,  3.4616e-03,\n",
      "         7.8085e-03, -7.2166e-02, -2.1875e-01, -3.4818e-01,  3.2477e-01,\n",
      "        -1.4292e-01, -1.2506e-01, -6.2996e-01,  6.3847e-01, -8.5184e-02,\n",
      "         2.4153e-02,  9.4701e-02,  5.3381e-01,  2.7650e-01,  1.6560e-01,\n",
      "        -1.3233e-02, -5.1727e-01,  4.9531e-01, -2.5114e-02, -1.9786e-01,\n",
      "        -3.6332e-01, -1.1358e-01, -1.4875e-01,  1.1506e-01, -1.2402e-01,\n",
      "        -6.9219e-01, -1.6855e-01,  2.5744e-01,  3.3411e-01,  9.7521e-02,\n",
      "        -2.6097e-02, -1.0443e-01,  1.7011e-03,  1.0796e-01, -3.3634e-01,\n",
      "        -1.9889e-01, -3.0458e-01,  2.0716e-01, -2.0506e-01, -2.6074e-02,\n",
      "         1.1937e-03, -2.8666e-01,  1.6621e-01, -5.5281e-02,  1.0792e-01,\n",
      "         4.0843e-01,  5.0578e-01,  3.2894e-01,  8.8766e-02, -3.8443e-01,\n",
      "        -4.8804e-01,  2.4851e-01, -1.0280e-01,  3.6984e-01,  9.4782e-02,\n",
      "         1.2793e-02, -4.1284e-01,  1.7184e-01, -1.2502e-02, -3.7925e-01,\n",
      "        -7.3700e-01, -3.6218e-02, -2.3542e-01, -1.2441e-01, -3.0826e-01,\n",
      "        -7.3929e-03,  6.5330e-01, -2.2563e-01,  3.7134e-01, -1.4614e-01,\n",
      "        -3.0144e-01, -2.5722e-01, -2.1087e-01,  8.2466e-02, -4.9029e-01,\n",
      "        -4.6970e-01, -9.9760e-02, -1.8981e-01, -3.8287e-01,  2.1769e-01,\n",
      "        -1.0006e-01,  3.6078e-01,  4.7703e-03, -1.2844e-01, -3.0801e-01,\n",
      "         8.1773e-02, -2.9941e-01,  1.2593e-01, -1.0642e-01, -8.1216e-01,\n",
      "        -2.3703e-02,  4.5574e-02, -1.3861e-01, -2.4486e+00,  1.5609e-01,\n",
      "        -1.7582e-01, -3.7484e-01,  2.5418e-01,  1.9281e-01, -5.9287e-02,\n",
      "         4.0309e-03, -2.3133e-01, -8.4711e-02,  1.8930e-01, -1.5023e-01,\n",
      "        -5.7620e-02,  2.7881e-01, -1.4104e-01, -2.0794e-01, -3.0060e-01,\n",
      "        -3.1101e-01, -4.9448e-01,  6.2944e-01,  1.4634e-01,  2.9838e-01,\n",
      "         1.3397e-01, -1.3930e-01,  5.5057e-01, -1.7617e-03, -3.5480e-01,\n",
      "         4.5672e-02, -2.5234e-01,  2.0612e-01, -9.8946e-02, -3.7412e-01,\n",
      "        -2.4873e-02,  2.8723e-01,  2.2456e-03, -1.4949e-01, -3.3864e-02,\n",
      "        -7.8339e-02,  3.3966e-01, -3.0150e-01, -7.4973e-02, -1.6782e-01,\n",
      "         1.7660e-01, -3.5035e-01,  5.3583e-01, -1.2296e-01, -9.0627e-02,\n",
      "        -2.7091e-01, -1.2144e-01, -6.5867e-02, -3.0239e-02,  1.5765e-01,\n",
      "         1.7380e-01,  2.8906e-01,  2.0237e-01, -4.3213e-01,  4.7507e-01,\n",
      "         4.9523e-01, -1.5589e-01, -2.8017e-01,  1.8659e-01, -5.3540e-01,\n",
      "        -2.7726e-01, -2.3259e-01,  4.0601e-02, -2.5877e-01, -7.1654e-01,\n",
      "        -4.0653e-01, -6.0433e-02,  3.3504e-01,  3.0435e-01, -1.3535e-01,\n",
      "        -3.1808e-01, -7.8975e-01, -6.0090e-03, -1.1167e-01, -3.9343e-01,\n",
      "        -1.0055e-01,  1.7449e-01, -3.8738e-02, -1.8936e-01, -5.1280e-01,\n",
      "        -2.5344e-01, -1.0584e-01, -1.4510e-01, -3.0311e-01, -4.2127e-01,\n",
      "        -1.7206e-01, -1.9552e-02, -2.9383e-01,  6.5781e-01,  3.0604e-01,\n",
      "         4.7663e-01,  8.2818e-02,  5.7010e-02,  1.4283e-02,  4.4751e-01,\n",
      "        -2.3807e-01,  3.5801e-01,  1.6013e-01, -3.8920e-02, -9.9533e-02,\n",
      "         2.3139e-02,  2.8486e-01,  4.3028e-01,  1.8947e-01, -5.3243e-01,\n",
      "        -5.1024e-02,  2.4550e-01,  2.2247e-01, -2.0967e-01, -3.2499e-01,\n",
      "         6.5330e-01,  7.9406e-02, -1.1470e-01,  1.6855e-02,  3.7360e-01,\n",
      "         2.5204e-01, -1.3979e-02, -1.0655e-01, -3.6285e-02,  5.1562e-01,\n",
      "        -4.5693e-01,  3.2855e-01, -1.2283e-01, -1.8505e-01, -9.0343e-02,\n",
      "        -2.2562e-01, -4.9801e-01, -2.0892e-02, -1.2399e-01, -1.8966e-01,\n",
      "        -1.3640e-01,  1.7351e-01,  1.0157e-01, -2.3433e-01, -2.2381e-01,\n",
      "         2.5376e-03, -5.8770e-02,  2.5677e-01,  1.8987e-01,  6.1504e-02,\n",
      "        -9.2252e-02, -4.5667e-02, -2.4084e-01,  3.6565e-01, -7.5555e-02,\n",
      "        -5.6082e-02, -2.5377e-01,  2.2952e-01, -1.9615e-01, -3.7694e-01,\n",
      "        -1.0240e-01, -1.3229e-01,  2.1507e-01, -8.8495e-02,  2.0901e-01,\n",
      "         1.5172e-01,  5.8428e-02, -3.6855e-01,  3.5562e-01,  8.9099e-02,\n",
      "         1.7368e-01,  2.6490e-01,  7.4019e-02,  8.9038e-02, -2.7351e-01,\n",
      "         9.3989e-02, -2.8388e-01,  2.1172e-01,  1.1882e-01, -2.9163e-01,\n",
      "         7.2208e-02, -4.1457e-01,  6.2906e-02,  3.0210e-01, -2.1347e-01,\n",
      "        -1.8831e-01, -2.5437e-01,  4.2686e-01, -1.0348e-01, -5.5011e-01,\n",
      "        -1.9191e-01,  3.3194e-02,  5.5852e-01,  1.7760e-01,  2.7852e-01,\n",
      "         5.0169e-02,  1.0660e-01, -1.8983e-01, -3.6348e-01,  3.0103e-01,\n",
      "        -4.1311e-01, -6.0244e-01, -5.5592e-01, -4.4247e-02,  6.0265e-01,\n",
      "        -2.4470e-01,  1.9362e-01,  1.5001e-01,  5.3076e-01, -8.8664e-02,\n",
      "        -6.8961e-02,  2.8361e-02,  2.0842e-01,  2.7179e-01, -3.3166e-03,\n",
      "        -1.1726e-01, -3.5119e-02, -3.7922e-02, -4.2695e-01, -2.7571e-01,\n",
      "        -1.4926e-01, -6.5632e-01,  8.5236e-02,  1.8227e-01,  1.6398e-01,\n",
      "         1.0671e-01, -1.0271e-01,  6.4155e-02, -7.5259e-01,  1.4385e-01,\n",
      "        -1.8036e-01,  1.1994e-01, -2.6438e-01, -5.6172e-01, -1.8383e-01,\n",
      "        -3.6423e-02, -5.1509e-01,  3.4140e-01,  5.3867e-02, -4.2566e-02,\n",
      "         1.7699e-01, -1.4746e-02,  3.6671e-01,  3.1403e-01,  1.1446e-01,\n",
      "         7.6949e-03,  5.8589e-01, -2.1011e-01,  1.2202e-01, -1.4053e-01,\n",
      "        -2.8620e-01,  2.6387e-01,  3.4865e-01,  1.5595e-02, -1.5991e-01,\n",
      "         3.2822e-01, -2.6856e-01, -2.0744e-01,  1.4418e-01,  1.9791e-01,\n",
      "        -2.7928e-01,  7.1161e-02,  2.8712e-02, -8.3688e-01, -3.8637e-01,\n",
      "         1.3331e-01, -6.1655e-02,  2.7228e-01, -1.6065e-01, -5.0256e-02,\n",
      "        -4.0080e-01,  4.1891e-01,  2.2390e-01,  3.0004e-01, -2.4773e-01,\n",
      "         4.6381e-01,  1.3763e-01, -2.1192e-02, -3.1502e-01, -1.4956e-01,\n",
      "        -3.2016e-02,  1.2317e-01, -3.9904e-01, -1.3379e-01,  1.9379e-01,\n",
      "        -1.9716e-01,  2.5300e-02, -4.8948e-01,  4.1892e-01,  3.5867e-01,\n",
      "         3.3771e-01, -7.2635e-01,  1.6039e-01, -1.2585e-01, -3.6693e-01,\n",
      "         2.3894e-01, -3.2418e-01,  1.0038e-01,  2.5798e-01,  1.9640e-01,\n",
      "        -3.9649e-02,  1.8104e-01,  2.4999e-03,  1.5875e-01,  2.5390e-01,\n",
      "         1.3364e-01,  7.5818e-02,  1.1948e-02, -1.3136e-01,  2.9102e-01,\n",
      "         2.4504e-01, -1.9786e-01, -2.9360e-02,  1.4348e-01,  1.3882e-01,\n",
      "         5.2985e-02,  3.0414e-01,  2.1974e-01, -4.5804e-01,  2.7781e-02,\n",
      "         4.2195e-01,  4.3770e-01, -3.7451e-01, -1.4489e-01,  3.5282e-01,\n",
      "        -3.3329e-01,  2.2201e-03,  2.6189e-01,  1.4992e-01, -1.2569e-01,\n",
      "         3.4356e-01,  2.9686e-02,  2.6419e-02,  7.7172e-01, -5.4552e-01,\n",
      "        -2.9756e-02,  4.3536e-01,  2.7801e-01,  1.0568e-01,  5.6365e-01,\n",
      "        -3.6917e-01,  3.2696e-01, -7.6487e-02, -1.7832e-01,  1.0284e-02,\n",
      "         2.6711e-01,  3.0724e-01,  1.2860e-01,  3.6250e-01, -1.5245e-02,\n",
      "         2.2299e-02,  5.9863e-01, -2.7586e-02,  1.1135e-02,  3.2547e-01,\n",
      "         2.0309e-01,  4.0451e-01, -6.3751e-02,  3.0449e-01,  3.0733e-01,\n",
      "         4.6520e-01, -7.4356e-02, -1.1722e-01,  2.5692e-01,  3.7213e-01,\n",
      "         1.9190e-01,  3.3404e-01, -4.5438e-01,  1.3130e-01,  6.0920e-02,\n",
      "         1.2500e-01, -5.7841e-01,  3.0553e-01,  6.9051e-01,  2.9128e-01,\n",
      "        -3.7396e-01, -1.8412e-01, -1.4721e-01, -2.1122e-01, -5.0353e-02,\n",
      "        -2.3170e-02,  9.4406e-02, -1.4853e-02,  1.8439e-01, -2.4469e-01,\n",
      "         1.7061e-01, -1.8245e-01, -2.4902e-01,  3.3608e-01,  1.7128e-01,\n",
      "        -4.7941e-01, -2.1782e-01, -1.6610e-01, -2.3524e-01, -8.1452e-02,\n",
      "        -1.1041e-01,  7.2533e-02, -1.4243e-01, -3.1680e-02,  2.9804e-03,\n",
      "        -1.9198e-02, -5.6251e-01,  3.8515e-01, -5.3100e-01,  1.6484e-01,\n",
      "         3.0079e-01,  4.7441e-01, -3.0696e-02,  2.6004e-01, -6.1172e-02,\n",
      "        -1.1625e-01,  4.1823e-01, -2.1462e-01,  4.7369e-01, -3.5023e-02,\n",
      "         3.2239e-01, -3.2049e-01, -7.2751e-01,  1.3440e-01,  2.7377e-01,\n",
      "        -1.0651e+00,  1.5012e-01,  2.2101e-01, -1.9055e-02,  1.3275e-01,\n",
      "         7.9990e-02,  7.2443e-02,  2.4633e-02, -1.1977e-01, -2.4682e-01,\n",
      "         2.5973e-01,  5.2500e-01, -2.8096e-01,  1.1901e-01, -4.2601e-02,\n",
      "        -6.6570e-03, -7.8842e-02, -2.4973e-01, -3.0657e-02,  4.3044e-01,\n",
      "        -1.9565e-01, -2.6131e-01,  3.1867e-01,  4.4632e-02, -9.5151e-02,\n",
      "         1.1346e-01, -2.4264e-01, -2.1976e-01, -9.5688e-02, -5.5568e-02,\n",
      "        -7.6518e-02, -2.4131e-01, -1.3482e+00, -7.5482e-02, -2.7006e-02,\n",
      "        -6.7312e-01,  3.8559e-02, -7.5229e-01,  1.0116e-01, -1.7178e-01,\n",
      "         2.1387e-01, -8.4129e-02,  6.8001e-02,  1.3940e-01,  5.5383e-01,\n",
      "        -3.0818e-01, -1.0144e-01,  2.3432e-01])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.6471195816993713\n",
      "Epoch 2, Average Loss: 0.5051839351654053\n",
      "Epoch 3, Average Loss: 0.3805992901325226\n",
      "Test Accuracy: 100.00%\n",
      "Input Text: The movie is nice\n",
      "Predicted Label: positive\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# Input text for which you want to generate embeddings\n",
    "input_text = \"I find natural language processing amazing.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokenized_text = tokenizer.tokenize(input_text)\n",
    "tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Convert token IDs to tensor\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# Get the BERT model output\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "\n",
    "# Extract the embeddings from the BERT model outputs\n",
    "hidden_states = outputs[2]\n",
    "word_embeddings = hidden_states[-1]  # Last layer hidden states for each token\n",
    "\n",
    "# Average the token embeddings to get the sentence embedding\n",
    "sentence_embedding = torch.mean(word_embeddings, dim=1).squeeze()\n",
    "\n",
    "print(\"Word Embeddings:\")\n",
    "print(word_embeddings)\n",
    "print(\"Sentence Embedding:\")\n",
    "print(sentence_embedding)\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data for text classification\n",
    "texts = [\"I love natural language processing.\", \"This movie is great!\", \"I don't like this product.\", \"The weather today is nice.\"]\n",
    "labels = [1, 1, 0, 1]  # Binary labels (1: positive, 0: negative)\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 classes for binary classification\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "input_ids_train, input_ids_test, labels_train, labels_test = \\\n",
    "    train_test_split(tokenized_texts['input_ids'], labels, test_size=0.2, random_state=42)\n",
    "attention_masks_train, attention_masks_test = \\\n",
    "    train_test_split(tokenized_texts['attention_mask'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_data = TensorDataset(input_ids_train, attention_masks_train, torch.tensor(labels_train))\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(input_ids_test, attention_masks_test, torch.tensor(labels_test))\n",
    "test_loader = DataLoader(test_data, batch_size=4, shuffle=False)\n",
    "\n",
    "# Set optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Example: 3 epochs\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids_batch, attention_masks_batch, labels_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch, labels=labels_batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation on test data\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids_batch, attention_masks_batch, labels_batch = batch\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        true_labels.extend(labels_batch.tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = torch.sum(torch.tensor(predictions) == torch.tensor(true_labels)).item() / len(true_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Sample input text for testing\n",
    "input_text = \"The movie is nice\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokenized_input = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions using the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_input)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "predicted_label = \"positive\" if prediction == 1 else \"negative\"\n",
    "\n",
    "print(f\"Input Text: {input_text}\")\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
